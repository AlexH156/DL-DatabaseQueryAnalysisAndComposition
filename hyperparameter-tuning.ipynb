{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce4743b-537e-4160-abde-c25e56008734",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "import glob\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras import Model, layers\n",
    "from keras.layers import Dense, Embedding, Conv1D, GlobalMaxPooling1D, Dropout, Input, Flatten, MaxPooling1D, GlobalAveragePooling1D\n",
    "import random\n",
    "import logging\n",
    "from importlib import reload\n",
    "path=\"data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b34361a-108c-482b-a2ef-54c3ade02e30",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load Data & Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5618bf10-856a-4389-aa81-12d5d9ef5a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(f\"{path}train.csv\", on_bad_lines=\"skip\", encoding=\"latin-1\", lineterminator=\"\\n\",\n",
    "                                dtype={\"statement\": str, \"runtime\": float, \"resultsize\": int, \"yy\": int,\n",
    "                                       \"mm:\": int, \"dd\": int}, memory_map=True).sample(frac=0.2)\n",
    "print(len(data))\n",
    "\n",
    "# Tokenize input SQL statements\n",
    "tokenizer = Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(data[\"statement\"])\n",
    "    \n",
    "min_runtime = min(data[\"runtime\"])\n",
    "min_resultsize = min(data[\"resultsize\"])\n",
    "\n",
    "data_runtime = np.log(data[\"runtime\"] + 1 - min_runtime)\n",
    "data_resultsize = np.log(data[\"resultsize\"] + 1 - min_resultsize)\n",
    "\n",
    "# Pad input sequences\n",
    "max_len = 512\n",
    "sequences = tokenizer.texts_to_sequences(data[\"statement\"])\n",
    "padded = pad_sequences(sequences, maxlen=max_len, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80264cc3-beb6-408d-a444-7684b0eb9b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(run_dir, hparams):\n",
    "    with tf.summary.create_file_writer(run_dir).as_default():\n",
    "        hp.hparams(hparams)  # record the values used in this trial\n",
    "        loss = train_test_model(hparams)\n",
    "        tf.summary.scalar('loss', loss, step=1)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2015520f-280b-4973-98de-2c1cbc3e048e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d062f3d5-3466-4f10-a3fb-bfef4e17fa91",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d6281f-7f1d-4662-ac0b-aacc23c94d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboard.plugins.hparams import api as hp\n",
    "HP_NUM_FILTERS = hp.HParam('num_filters', hp.Discrete([64,128,256]))\n",
    "HP_FILTER_SIZE = hp.HParam('filter_size', hp.Discrete([3,5,7]))\n",
    "HP_HIDDEN_UNITS = hp.HParam('hidden_units', hp.Discrete([64,128,256]))\n",
    "HP_BATCH_SIZE = hp.HParam('batch_size', hp.Discrete([1024]))#16,32,64\n",
    "HP_EMBED_DIM = hp.HParam('embed_dim', hp.Discrete([128,256,512]))\n",
    "HP_DROPOUT = hp.HParam('dropout', hp.Discrete([0.3,0.5,0.7]))\n",
    "HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam','adamax']))\n",
    "HP_EPOCHS = hp.HParam('epochs', hp.Discrete([20]))\n",
    "HP_LENGTH = hp.HParam('length', hp.Discrete([512]))\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "with tf.summary.create_file_writer('runs/hparam_tuning/cnn').as_default():\n",
    "    hp.hparams_config(\n",
    "        hparams=[HP_NUM_FILTERS, HP_FILTER_SIZE, HP_HIDDEN_UNITS, HP_BATCH_SIZE, HP_EMBED_DIM, HP_DROPOUT, HP_OPTIMIZER, HP_LENGTH, HP_EPOCHS],\n",
    "        metrics=[hp.Metric('loss', display_name='loss')],\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c379109d-bdff-4019-b65b-0edf5b8eef17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_model(hparams):\n",
    "    max_len=hparams[HP_LENGTH]\n",
    "    EPOCHS=hparams[HP_EPOCHS]\n",
    "    BATCH_SIZE=hparams[HP_BATCH_SIZE]\n",
    "\n",
    "    il = Input(shape=(max_len,))\n",
    "    el = Embedding(vocab_size, hparams[HP_EMBED_DIM], input_length=max_len)(il)\n",
    "    cl = Conv1D(hparams[HP_NUM_FILTERS], hparams[HP_FILTER_SIZE], activation='relu')(el)\n",
    "    pl = GlobalMaxPooling1D()(cl)\n",
    "    hl = Dense(hparams[HP_HIDDEN_UNITS], activation='relu')(pl)\n",
    "    outTime = Dense(1, name=\"outTime\")(hl)\n",
    "    outSize = Dense(1, name=\"outSize\")(hl)\n",
    "\n",
    "    model = Model(inputs=il, outputs=[outTime, outSize])\n",
    "    model.compile(optimizer=hparams[HP_OPTIMIZER], loss={\"outTime\": 'huber', \"outSize\": \"huber\"})\n",
    "\n",
    "    result = model.fit(padded, [data_runtime, data_resultsize],\n",
    "          epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=0,\n",
    "      shuffle=True,\n",
    "      validation_split=0.2)\n",
    "\n",
    "    return result.history['val_loss'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae27832-daea-4a92-994b-238435e4d906",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(logging)\n",
    "logging.basicConfig(filename='logs/ccnnHyper.log', level=logging.DEBUG, format=\"%(asctime)s    %(message)s\",\n",
    "                              datefmt=\"%H:%M\")\n",
    "session_num = 0\n",
    "\n",
    "for max_len in HP_LENGTH.domain.values:\n",
    "    for filter_size in HP_FILTER_SIZE.domain.values:\n",
    "        for hidden_units in HP_HIDDEN_UNITS.domain.values:\n",
    "            for batch_size in HP_BATCH_SIZE.domain.values:\n",
    "                for embed_dim in HP_EMBED_DIM.domain.values:\n",
    "                    for dropout_rate in HP_DROPOUT.domain.values:\n",
    "                        for optimizer in HP_OPTIMIZER.domain.values:\n",
    "                            # logging.info(session_num)\n",
    "                            for epochs in HP_EPOCHS.domain.values:\n",
    "                                for num_filters in HP_NUM_FILTERS.domain.values:\n",
    "                                    hparams = {\n",
    "                                        HP_NUM_FILTERS: num_filters,\n",
    "                                        HP_FILTER_SIZE: filter_size,\n",
    "                                        HP_HIDDEN_UNITS: hidden_units,\n",
    "                                        HP_BATCH_SIZE: batch_size,\n",
    "                                        HP_EMBED_DIM: embed_dim,\n",
    "                                        HP_DROPOUT: dropout_rate,\n",
    "                                        HP_OPTIMIZER: optimizer,\n",
    "                                        HP_EPOCHS: epochs,\n",
    "                                        HP_LENGTH: max_len,\n",
    "                                      }\n",
    "                                    run_name = \"run-%d\" % session_num\n",
    "                                    loss = run('runs/hparam_tuning/cnn/' + run_name, hparams)\n",
    "                                    logging.info({h.name: hparams[h] for h in hparams})\n",
    "                                    logging.info(loss)\n",
    "                                    session_num += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca813008-9d40-4daf-9d76-a30df86d400e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## VDCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a8fdd1-6907-457a-bb8e-b84cd518bc32",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49afeb06-c804-46a9-9c91-6f41d2299e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_BLOCKS = {9: (1, 1, 1, 1),\n",
    "            17: (2, 2, 2, 2),\n",
    "            29: (5, 5, 2, 2),\n",
    "            49: (8, 8, 5, 3)}\n",
    "\n",
    "\n",
    "class KMaxPooling(layers.Layer):\n",
    "    \"\"\"\n",
    "    K-max pooling layer that extracts the k-highest activations from a sequence (2nd dimension).\n",
    "    TensorFlow backend.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 k=None,\n",
    "                 sorted=False):\n",
    "        super(KMaxPooling, self).__init__()\n",
    "        self.k = k\n",
    "        self.sorted = sorted\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.k, input_shape[2])\n",
    "\n",
    "    def call(self,\n",
    "             inputs):\n",
    "        if self.k is None:\n",
    "            k = int(tf.round(inputs.shape[1] / 2))\n",
    "        else:\n",
    "            k = self.k\n",
    "\n",
    "        # Swap last two dimensions since top_k will be applied along the last dimension\n",
    "        shifted_inputs = tf.transpose(inputs, [0, 2, 1])\n",
    "\n",
    "        # Extract top_k, returns two tensors [values, indices]\n",
    "        top_k = tf.nn.top_k(shifted_inputs, k=k, sorted=self.sorted)[0]\n",
    "\n",
    "        # return flattened output\n",
    "        return tf.transpose(top_k, [0, 2, 1])\n",
    "\n",
    "\n",
    "class Pooling(layers.Layer):\n",
    "    \"\"\"Wrapper for different pooling operations.\n",
    "    Including maxpooling and k-maxpooling.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 pool_type='max',\n",
    "                 name=None):\n",
    "        super(Pooling, self).__init__(name=name)\n",
    "        assert pool_type in ['max', 'k_max']\n",
    "        self.pool_type = pool_type\n",
    "\n",
    "        if pool_type == 'max':\n",
    "            self.pool = layers.MaxPooling1D(pool_size=3, strides=2, padding='same')\n",
    "        elif pool_type == 'k_max':\n",
    "            self.pool = KMaxPooling()\n",
    "\n",
    "    def call(self,\n",
    "             x):\n",
    "        return self.pool(x)\n",
    "\n",
    "\n",
    "class ZeroPadding(layers.Layer):\n",
    "    def __init__(self,\n",
    "                 values,\n",
    "                 name=None):\n",
    "        super(ZeroPadding, self).__init__(name=name)\n",
    "        self.values = values\n",
    "\n",
    "    def call(self,\n",
    "             x):\n",
    "        x = tf.pad(x, [[0, 0], [0, 0], [self.values[0], self.values[1]]],\n",
    "                   mode='CONSTANT', constant_values=0)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Conv1D_BN(layers.Layer):\n",
    "    \"\"\"A stack of conv 1x1 and BatchNorm.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 filters,\n",
    "                 kernel_size=3,\n",
    "                 strides=2,\n",
    "                 padding='same',\n",
    "                 use_bias=True,\n",
    "                 name=None):\n",
    "        super(Conv1D_BN, self).__init__(name=name)\n",
    "        self.filters = filters\n",
    "        self.use_bias = use_bias\n",
    "        self.conv = layers.Conv1D(filters, kernel_size, strides=strides, padding=padding, use_bias=use_bias,\n",
    "                                  kernel_initializer='he_normal')\n",
    "        self.bn = layers.BatchNormalization()\n",
    "\n",
    "    def call(self,\n",
    "             x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ConvBlock(layers.Layer):\n",
    "    \"\"\"Conv block with downsampling.\n",
    "    1x1 conv to increase dimensions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 filters,\n",
    "                 kernel_size=3,\n",
    "                 use_bias=True,\n",
    "                 shortcut=True,\n",
    "                 pool_type=None,\n",
    "                 proj_type=None,\n",
    "                 name=None,\n",
    "                 ):\n",
    "        super(ConvBlock, self).__init__(name=name)\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.use_bias = use_bias\n",
    "        self.shortcut = shortcut\n",
    "        self.pool_type = pool_type\n",
    "        self.proj_type = proj_type\n",
    "\n",
    "        # Deal with downsample and pooling\n",
    "        assert pool_type in ['max', 'k_max', 'conv', None]\n",
    "        if pool_type is None:\n",
    "            strides = 1\n",
    "            self.pool = None\n",
    "            self.downsample = None\n",
    "\n",
    "        elif pool_type == 'conv':\n",
    "            strides = 2  # Convolutional pooling with stride 2\n",
    "            self.pool = None\n",
    "            if shortcut:\n",
    "                self.downsample = Conv1D_BN(filters, 3, strides=2, padding='same', use_bias=use_bias)\n",
    "\n",
    "        else:\n",
    "            strides = 1\n",
    "            self.pool = Pooling(pool_type)\n",
    "            if shortcut:\n",
    "                self.downsample = Conv1D_BN(filters, 3, strides=2, padding='same', use_bias=use_bias)\n",
    "\n",
    "        self.conv1 = layers.Conv1D(filters, kernel_size, strides=strides, padding='same', use_bias=use_bias,\n",
    "                                   kernel_initializer='he_normal')\n",
    "        self.bn1 = layers.BatchNormalization()\n",
    "\n",
    "        self.conv2 = layers.Conv1D(filters, kernel_size, strides=1, padding='same', use_bias=use_bias,\n",
    "                                   kernel_initializer='he_normal')\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "\n",
    "        assert proj_type in ['identity', 'conv', None]\n",
    "        if shortcut:\n",
    "            if proj_type == 'conv':\n",
    "                # 1x1 conv for projection\n",
    "                self.proj = Conv1D_BN(filters * 2, 1, strides=1, padding='same', use_bias=use_bias)\n",
    "\n",
    "            elif proj_type == 'identity':\n",
    "                # Identity using zero padding\n",
    "                self.proj = ZeroPadding([int(filters // 2), filters - int(filters // 2)])\n",
    "\n",
    "    def call(self,\n",
    "             x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = tf.nn.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.pool is not None:\n",
    "            out = self.pool(out)\n",
    "\n",
    "        if self.shortcut:\n",
    "            if self.downsample is not None:\n",
    "                residual = self.downsample(residual)\n",
    "            out += residual\n",
    "\n",
    "        out = tf.nn.relu(out)\n",
    "\n",
    "        if self.proj_type is not None and self.shortcut:\n",
    "            out = self.proj(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class VDCNN(Model):\n",
    "    \"\"\"Model codebase for VDCNN.\n",
    "    Args:\n",
    "        depth: depth of VDCNN, one of [9, 17, 29, 49].\n",
    "        seqlen: Sequence length.\n",
    "        embed_dim: dim for character embeddings.\n",
    "        shortcut: Use skip connections.\n",
    "        pool_type: Pooling operations to be used, one of ['max', 'k_max', 'conv'].\n",
    "        proj_type: Operation to increase dim for dotted skip connection, one of ['identity', 'conv'].\n",
    "        use_bias: Use bias for all layers or not.\n",
    "        logits: If False, return softmax probs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 depth=9,\n",
    "                 vocab_size=69,\n",
    "                 seqlen=None,\n",
    "                 embed_dim=16,\n",
    "                 shortcut=True,\n",
    "                 pool_type='max',\n",
    "                 proj_type='conv',\n",
    "                 use_bias=True,\n",
    "                 logits=True):\n",
    "        super(VDCNN, self).__init__()\n",
    "        self.depth = depth\n",
    "        self.vocab_size = vocab_size\n",
    "        self.seqlen = seqlen\n",
    "        self.embed_dim = embed_dim\n",
    "        self.shortcut = shortcut\n",
    "        self.pool_type = pool_type\n",
    "        self.proj_type = proj_type\n",
    "        self.use_bias = use_bias\n",
    "        self.logits = True\n",
    "\n",
    "        assert pool_type in ['max', 'k_max', 'conv']\n",
    "        assert proj_type in ['conv', 'identity']\n",
    "        self.n_blocks = N_BLOCKS[depth]\n",
    "\n",
    "        self.embed_char = layers.Embedding(vocab_size, embed_dim, input_length=seqlen)\n",
    "        self.conv = layers.Conv1D(64, 3, strides=1, padding='same', use_bias=use_bias,\n",
    "                                  kernel_initializer='he_normal')\n",
    "\n",
    "        # Convolutional Block 64\n",
    "        self.conv_block_64 = []\n",
    "        for _ in range(self.n_blocks[0] - 1):\n",
    "            self.conv_block_64.append(ConvBlock(64, 3, use_bias, shortcut))\n",
    "        self.conv_block_64.append(ConvBlock(64, 3, use_bias, shortcut, pool_type=pool_type, proj_type=proj_type))\n",
    "\n",
    "        # Convolutional Block 128\n",
    "        self.conv_block_128 = []\n",
    "        for _ in range(self.n_blocks[1] - 1):\n",
    "            self.conv_block_128.append(ConvBlock(128, 3, use_bias, shortcut))\n",
    "        self.conv_block_128.append(ConvBlock(128, 3, use_bias, shortcut, pool_type=pool_type, proj_type=proj_type))\n",
    "\n",
    "        # Convolutional Block 256\n",
    "        self.conv_block_256 = []\n",
    "        for _ in range(self.n_blocks[2] - 1):\n",
    "            self.conv_block_256.append(ConvBlock(256, 3, use_bias, shortcut))\n",
    "        self.conv_block_256.append(ConvBlock(256, 3, use_bias, shortcut, pool_type=pool_type, proj_type=proj_type))\n",
    "\n",
    "        # Convolutional Block 512\n",
    "        self.conv_block_512 = []\n",
    "        for _ in range(self.n_blocks[3] - 1):\n",
    "            self.conv_block_512.append(ConvBlock(512, 3, use_bias, shortcut))\n",
    "        self.conv_block_512.append(ConvBlock(512, 3, use_bias, shortcut, pool_type=None, proj_type=None))\n",
    "\n",
    "        self.k_maxpool = KMaxPooling(k=8)\n",
    "        self.flatten = layers.Flatten()\n",
    "\n",
    "        # Dense layers\n",
    "        self.fc1 = layers.Dense(2048, activation='relu')\n",
    "        self.fc2 = layers.Dense(2048, activation='relu')\n",
    "        self.outTime = layers.Dense(1, name=\"outTime\")\n",
    "        self.outSize = layers.Dense(1, name=\"outSize\")\n",
    "        # self.out = layers.Dense(2)\n",
    "\n",
    "    def call(self,\n",
    "             x):\n",
    "        x = self.embed_char(x)\n",
    "        # print('embed:', x.shape)\n",
    "        x = self.conv(x)\n",
    "        # print('conv:', x.shape)\n",
    "\n",
    "        for l in self.conv_block_64:\n",
    "            x = l(x)\n",
    "        # print('conv_block_64:', x.shape)\n",
    "\n",
    "        for l in self.conv_block_128:\n",
    "            x = l(x)\n",
    "        # print('conv_block_128:', x.shape)\n",
    "\n",
    "        for l in self.conv_block_256:\n",
    "            x = l(x)\n",
    "        # print('conv_block_256:', x.shape)\n",
    "\n",
    "        for l in self.conv_block_512:\n",
    "            x = l(x)\n",
    "        # print('conv_block_512:', x.shape)\n",
    "\n",
    "        x = self.k_maxpool(x)\n",
    "        # print('k_maxpool_8:', x.shape)\n",
    "        x = self.flatten(x)\n",
    "        # print('flatten:', x.shape)\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        outTime = self.outTime(x)\n",
    "        outSize = self.outSize(x)\n",
    "        # out = self.out(x)\n",
    "        # print('out:', out.shape)\n",
    "\n",
    "        return outTime, outSize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cbcca1-b36b-42b7-9678-7835f5be4463",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e02371-bad1-4665-8b56-9a7211ad5bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboard.plugins.hparams import api as hp\n",
    "HP_DEPTH = hp.HParam('depth', hp.Discrete([49])) # 9,17,29,49\n",
    "HP_EMBED_DIM = hp.HParam('embed_dim', hp.Discrete([512,256]))#512,128,256\n",
    "HP_BATCH_SIZE = hp.HParam('batch_size', hp.Discrete([256]))#16,32,64,128\n",
    "HP_SHORTCUT = hp.HParam('shortcut', hp.Discrete([True, False])) # True, False\n",
    "HP_POOL_TYPE = hp.HParam('pool_type', hp.Discrete(['conv','k_max'])) # 'conv','max','k_max'\n",
    "HP_PROJ_TYPE = hp.HParam('proj_type', hp.Discrete(['identity','conv'])) # 'identity','conv'\n",
    "HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adamax','adam'])) # 'adamax','adam','sgd'\n",
    "HP_BIAS = hp.HParam('bias', hp.Discrete([False,True])) # True,False\n",
    "HP_EPOCHS = hp.HParam('epochs', hp.Discrete([20]))\n",
    "HP_LENGTH = hp.HParam('length', hp.Discrete([512]))\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "run_dir = 'runs/hparam_tuning/vdcnn29/'\n",
    "\n",
    "with tf.summary.create_file_writer(run_dir).as_default():\n",
    "    hp.hparams_config(\n",
    "        hparams=[HP_DEPTH, HP_EMBED_DIM, HP_BATCH_SIZE, HP_SHORTCUT, HP_POOL_TYPE, HP_PROJ_TYPE, HP_OPTIMIZER, HP_BIAS, HP_EPOCHS, HP_LENGTH],\n",
    "        metrics=[hp.Metric('loss', display_name='Loss')],\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01022da7-f82d-4644-ad5a-d229d65a60ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(logging)\n",
    "logging.basicConfig(filename='logs/vdcnnHyper.log', level=logging.DEBUG, format=\"%(asctime)s    %(message)s\",\n",
    "                              datefmt=\"%H:%M\")\n",
    "\n",
    "def train_test_model(hparams):\n",
    "    max_len=hparams[HP_LENGTH]\n",
    "    EPOCHS=hparams[HP_EPOCHS]\n",
    "    BATCH_SIZE=hparams[HP_BATCH_SIZE]\n",
    "    \n",
    "    # Model\n",
    "    model = VDCNN(depth=hparams[HP_DEPTH],\n",
    "                  vocab_size=vocab_size,\n",
    "                  seqlen=max_len,\n",
    "                  embed_dim=hparams[HP_EMBED_DIM],\n",
    "                  shortcut=hparams[HP_SHORTCUT],\n",
    "                  pool_type=hparams[HP_POOL_TYPE],\n",
    "                  proj_type=hparams[HP_PROJ_TYPE],\n",
    "                  use_bias=hparams[HP_BIAS])\n",
    "\n",
    "    # output_1=time, output_2=card\n",
    "    model.compile(optimizer=hparams[HP_OPTIMIZER], \n",
    "                  loss={\"output_1\": 'huber', \"output_2\": \"huber\"})\n",
    "    result = model.fit(padded, [data_runtime, data_resultsize],\n",
    "          epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=0,\n",
    "      shuffle=True,\n",
    "      validation_split=0.2)\n",
    "    return result.history['val_loss'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8b5992-1f13-4c5b-a53d-b9bbce285e9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session_num = 0\n",
    "\n",
    "for max_len in HP_LENGTH.domain.values:\n",
    "    for embed_dim in HP_EMBED_DIM.domain.values:\n",
    "        for shortcut in HP_SHORTCUT.domain.values:\n",
    "            for batch_size in HP_BATCH_SIZE.domain.values:\n",
    "                for pool_type in HP_POOL_TYPE.domain.values:\n",
    "                    for optimizer in HP_OPTIMIZER.domain.values:\n",
    "                        for proj_type in HP_PROJ_TYPE.domain.values:\n",
    "                            for bias in HP_BIAS.domain.values:\n",
    "                                for length in HP_LENGTH.domain.values:\n",
    "                                    for epochs in HP_EPOCHS.domain.values:\n",
    "                                        for depth in HP_DEPTH.domain.values:\n",
    "                                            hparams = {\n",
    "                                                HP_DEPTH: depth,\n",
    "                                                HP_EMBED_DIM: embed_dim,\n",
    "                                                HP_SHORTCUT: shortcut,\n",
    "                                                HP_BATCH_SIZE: batch_size,\n",
    "                                                HP_POOL_TYPE: pool_type,\n",
    "                                                HP_OPTIMIZER: optimizer,\n",
    "                                                HP_PROJ_TYPE: proj_type,\n",
    "                                                HP_BIAS: bias,\n",
    "                                                HP_LENGTH: max_len,\n",
    "                                                HP_EPOCHS: epochs,\n",
    "                                              }\n",
    "                                            run_name = \"run-%d\" % session_num\n",
    "                                            # logging.info('--- Starting trial: %s' % run_name)     \n",
    "                                            loss = run(run_dir + run_name, hparams)\n",
    "                                            logging.info({h.name: hparams[h] for h in hparams})\n",
    "                                            logging.info(loss)\n",
    "                                            session_num += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
