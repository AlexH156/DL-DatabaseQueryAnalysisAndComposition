{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3fd5d342-962a-4817-86a5-155e0eb15b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import dirname, abspath, join, exists\n",
    "import os\n",
    "import numpy as np\n",
    "from random import seed\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import HuberRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.losses import Huber\n",
    "import pandas as pd\n",
    "import logging\n",
    "from importlib import reload\n",
    "import pickle\n",
    "import time\n",
    "path=\"data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efbe3d3-5c78-43e1-9d15-40c1140226ef",
   "metadata": {
    "tags": []
   },
   "source": [
    "# C-TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "acdd8853-d5eb-4fca-9ba4-b8ec28be21d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "modeltype=\"resultsize\" # one of \"runtime\" or \"resultsize\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88840280-5c22-4276-a8b5-ea4aa03b244e",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(logging)\n",
    "logging.basicConfig(filename='logs/tfidf.log', level=logging.DEBUG, format=\"%(asctime)s    %(message)s\",\n",
    "                              datefmt=\"%H:%M\")\n",
    "\n",
    "train_dataset = pd.read_csv(f\"{path}train.csv\", on_bad_lines=\"skip\", encoding=\"latin-1\", lineterminator=\"\\n\",\n",
    "                        dtype={\"statement\": str, \"runtime\": float, \"resultsize\": int, \"yy\": int,\n",
    "                               \"mm:\": int, \"dd\": int}, memory_map=True)\n",
    "tfidf = TfidfVectorizer(analyzer=\"char\", ngram_range=(1,5), max_features=500000)\n",
    "x_transformed = tfidf.fit_transform(train_dataset[\"statement\"])\n",
    "pickle.dump(tfidf, open(f\"models/ctfidf_{modeltype}.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2c699a-19cb-4d74-87dd-1582834a4c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HuberRegressor(max_iter=10000)\n",
    "min_modeltype = min(train_dataset[modeltype])\n",
    "pred_modeltype = np.log(train_dataset[modeltype] + 1 - min_modeltype)\n",
    "model.fit(x_transformed, pred_modeltype)\n",
    "\n",
    "logging.info(f\"Fit done for {modeltype}. Starting now with evaluation\")\n",
    "pickle.dump(model, open(f\"models/{modeltype}.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d5ed8872-136b-47c0-9001-08ca81f17105",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pickle.load(open(f\"models/{modeltype}.pkl\", \"rb\"))\n",
    "tfidf = pickle.load(open(f\"models/ctfidf_{modeltype}.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aa1e0502-5456-4308-b7dd-50b5bb9a0403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation\n"
     ]
    }
   ],
   "source": [
    "reload(logging)\n",
    "logging.basicConfig(filename='logs/tfidf.log', level=logging.DEBUG, format=\"%(asctime)s    %(message)s\",\n",
    "                              datefmt=\"%H:%M\")\n",
    "\n",
    "print(\"Starting evaluation\")\n",
    "test_dataset = pd.read_csv(f\"{path}test.csv\", on_bad_lines=\"skip\", encoding=\"latin-1\", lineterminator=\"\\n\",\n",
    "                        dtype={\"statement\": str, \"runtime\": float, \"resultsize\": int, \"yy\": int,\n",
    "                               \"mm:\": int, \"dd\": int}, memory_map=True)\n",
    "\n",
    "x_test_transformed = tfidf.transform(test_dataset[\"statement\"])\n",
    "pred = model.predict(x_test_transformed)\n",
    "\n",
    "min_modeltype = min(test_dataset[modeltype])\n",
    "target_data = np.log(test_dataset[modeltype] + 1 - min_modeltype)\n",
    "\n",
    "mse = mean_squared_error(target_data, pred)\n",
    "mae = mean_absolute_error(target_data, pred)\n",
    "\n",
    "h = Huber()\n",
    "test_loss = h(target_data,pred).numpy()\n",
    "\n",
    "logging.info(f\"TFIDF. {modeltype}. Test loss: {test_loss}, MSE {modeltype}: {mse}, MAE {modeltype}: {mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa2d3ce-a9b4-4da6-aa2e-09f132c24e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(logging)\n",
    "logging.basicConfig(filename='logs/tfidf.log', level=logging.DEBUG, format=\"%(asctime)s    %(message)s\",\n",
    "                              datefmt=\"%H:%M\")\n",
    "pred = [np.exp(x)-1+min_modeltype for x in pred]\n",
    "if modeltype==\"resultsize\":\n",
    "    qerror = [max(max(pred[i],1) / max(1,target_data[i]), max(1,target_data[i]) / max(1,pred[i])) for i in range(len(target_data))]\n",
    "else:\n",
    "    qerror = [max(pred[i] / target_data[i], target_data[i] / pred[i]) for i in range(len(target_data))]\n",
    "\n",
    "logging.info(\"\")\n",
    "logging.info(f\"Qerror for {modeltype}\")\n",
    "logging.info(\"Median: {}\".format(np.median(qerror)))\n",
    "logging.info(\"Mean: {}\".format(np.mean(qerror)))\n",
    "logging.info(\"Max: {}\".format(np.max(qerror)))\n",
    "logging.info(\"10th percentile: {}\".format(np.percentile(qerror, 10)))\n",
    "logging.info(\"20th percentile: {}\".format(np.percentile(qerror, 20)))\n",
    "logging.info(\"30th percentile: {}\".format(np.percentile(qerror, 30)))\n",
    "logging.info(\"40th percentile: {}\".format(np.percentile(qerror, 40)))\n",
    "logging.info(\"50th percentile: {}\".format(np.percentile(qerror, 50)))\n",
    "logging.info(\"60th percentile: {}\".format(np.percentile(qerror, 60)))\n",
    "logging.info(\"70th percentile: {}\".format(np.percentile(qerror, 70)))\n",
    "logging.info(\"80th percentile: {}\".format(np.percentile(qerror, 80)))\n",
    "logging.info(\"90th percentile: {}\".format(np.percentile(qerror, 90)))\n",
    "logging.info(\"95th percentile: {}\".format(np.percentile(qerror, 95)))\n",
    "logging.info(\"98th percentile: {}\".format(np.percentile(qerror, 98)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0c56ea-91dc-4c06-910e-03ac02b269f2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921afc34-1146-4500-a90e-209cac70a99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(logging)\n",
    "logging.basicConfig(filename='logs/median.log', level=logging.DEBUG, format=\"%(asctime)s    %(message)s\",\n",
    "                              datefmt=\"%H:%M\")\n",
    "\n",
    "# Calculate Medians\n",
    "train_dataset = pd.read_csv(f\"{path}train.csv\", on_bad_lines=\"skip\", encoding=\"latin-1\", lineterminator=\"\\n\",\n",
    "                        dtype={\"statement\": str, \"runtime\": float, \"resultsize\": int, \"yy\": int,\n",
    "                               \"mm:\": int, \"dd\": int}, memory_map=True)\n",
    "\n",
    "min_runtime = min(train_dataset[\"runtime\"])\n",
    "min_resultsize = min(train_dataset[\"resultsize\"])\n",
    "\n",
    "pred_time = np.median(train_dataset[\"runtime\"])\n",
    "pred_size = np.median(train_dataset[\"resultsize\"])\n",
    "\n",
    "pred_time = np.log(pred_time + 1 - min_runtime)\n",
    "pred_size = np.log(pred_size + 1 - min_resultsize)\n",
    "\n",
    "print(f\"Log Median Time: {pred_time}\")\n",
    "print(f\"Log Median Size: {pred_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "294f2bec-e454-4ec6-9273-49ae1f1402c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(logging)\n",
    "logging.basicConfig(filename='logs/median.log', level=logging.DEBUG, format=\"%(asctime)s    %(message)s\",\n",
    "                              datefmt=\"%H:%M\")\n",
    "\n",
    "# pred_time = 0.009949572809527275\n",
    "# pred_size = 2.9444389791664403\n",
    "logging.info(f\"Starting the test-process for Median.\")\n",
    "\n",
    "test_dataset = pd.read_csv(f\"{path}test.csv\", on_bad_lines=\"skip\", encoding=\"latin-1\", lineterminator=\"\\n\",\n",
    "                        dtype={\"statement\": str, \"runtime\": float, \"resultsize\": int, \"yy\": int,\n",
    "                               \"mm:\": int, \"dd\": int}, memory_map=True)\n",
    "\n",
    "min_runtime = min(test_dataset[\"runtime\"])\n",
    "min_resultsize = min(test_dataset[\"resultsize\"])\n",
    "\n",
    "target_time = np.log(test_dataset[\"runtime\"] + 1 - min_runtime)\n",
    "target_size = np.log(test_dataset[\"resultsize\"] + 1 - min_resultsize)\n",
    "\n",
    "count = len(test_dataset)\n",
    "\n",
    "mse_runtime = mean_squared_error(target_time,[pred_time]*count)\n",
    "mse_resultsize = mean_squared_error(target_size,[pred_size]*count)\n",
    "mae_runtime = mean_absolute_error(target_time,[pred_time]*count)\n",
    "mae_resultsize = mean_absolute_error(target_size,[pred_size]*count)\n",
    "\n",
    "h = Huber()\n",
    "test_loss_runtime = h(target_time,[pred_time]*count).numpy()\n",
    "test_loss_resultsize = h(target_size,[pred_size]*count).numpy()\n",
    "\n",
    "logging.info(f\"Median. Test loss runtime: {test_loss_runtime}, Test loss resultsize: {test_loss_resultsize}, overall loss: {test_loss_runtime+test_loss_resultsize}, \"\n",
    "             f\"MSE Runtime: {mse_runtime}, MSE Resultsize: {mse_resultsize}, \"\n",
    "             f\"MAE Runtime: {mae_runtime}, MAE Resultsize: {mae_resultsize}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352f4e30-6d92-4368-8c25-3a7f0871f7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual predictions\n",
    "pred_time = [np.median(test_dataset[\"runtime\"])]*count\n",
    "pred_size = [np.median(test_dataset[\"resultsize\"])]*count\n",
    "\n",
    "qerror_time = [max(pred_time[i] / test_dataset[\"runtime\"][i], test_dataset[\"runtime\"][i] / pred_time[i]) for i in range(len(test_dataset[\"runtime\"]))]\n",
    "qerror_size = [max(max(pred_size[i],1) / max(1,test_dataset[\"resultsize\"][i]), max(1,test_dataset[\"resultsize\"][i]) / max(1,pred_size[i])) for i in range(len(test_dataset[\"resultsize\"]))]\n",
    "\n",
    "for (qerror,name) in [(qerror_time,\"runtime\"), (qerror_size,\"resultsize\")]:\n",
    "    logging.info(\"\")\n",
    "    logging.info(f\"Qerror for {name}\")\n",
    "    logging.info(\"Median: {}\".format(np.median(qerror)))\n",
    "    logging.info(\"Mean: {}\".format(np.mean(qerror)))\n",
    "    logging.info(\"Max: {}\".format(np.max(qerror)))\n",
    "    logging.info(\"10th percentile: {}\".format(np.percentile(qerror, 10)))\n",
    "    logging.info(\"20th percentile: {}\".format(np.percentile(qerror, 20)))\n",
    "    logging.info(\"30th percentile: {}\".format(np.percentile(qerror, 30)))\n",
    "    logging.info(\"40th percentile: {}\".format(np.percentile(qerror, 40)))\n",
    "    logging.info(\"50th percentile: {}\".format(np.percentile(qerror, 50)))\n",
    "    logging.info(\"60th percentile: {}\".format(np.percentile(qerror, 60)))\n",
    "    logging.info(\"70th percentile: {}\".format(np.percentile(qerror, 70)))\n",
    "    logging.info(\"80th percentile: {}\".format(np.percentile(qerror, 80)))\n",
    "    logging.info(\"90th percentile: {}\".format(np.percentile(qerror, 90)))\n",
    "    logging.info(\"95th percentile: {}\".format(np.percentile(qerror, 95)))\n",
    "    logging.info(\"98th percentile: {}\".format(np.percentile(qerror, 98)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
