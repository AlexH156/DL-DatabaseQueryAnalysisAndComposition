{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ce4743b-537e-4160-abde-c25e56008734",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "import glob\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras import Model, layers\n",
    "from keras.layers import Dense, Embedding, Conv1D, GlobalMaxPooling1D, Dropout, Input, Flatten, MaxPooling1D, GlobalAveragePooling1D, Reshape, Conv2D, MaxPool2D, Concatenate\n",
    "import random\n",
    "import logging\n",
    "from importlib import reload\n",
    "path=\"data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b34361a-108c-482b-a2ef-54c3ade02e30",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5618bf10-856a-4389-aa81-12d5d9ef5a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24414\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(f\"{path}train.csv\", on_bad_lines=\"skip\", encoding=\"latin-1\", lineterminator=\"\\n\",\n",
    "                                dtype={\"statement\": str, \"runtime\": float, \"resultsize\": int, \"yy\": int,\n",
    "                                       \"mm:\": int, \"dd\": int}, memory_map=True)\n",
    "print(len(data))\n",
    "\n",
    "# Tokenize input SQL statements\n",
    "tokenizer = Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(data[\"statement\"])\n",
    "\n",
    "# Pad input sequences\n",
    "max_len = 512\n",
    "sequences = tokenizer.texts_to_sequences(data[\"statement\"])\n",
    "padded = pad_sequences(sequences, maxlen=max_len, padding='post', truncating='post')\n",
    "\n",
    "# Log regression data\n",
    "min_runtime = min(data[\"runtime\"])\n",
    "min_resultsize = min(data[\"resultsize\"])\n",
    "\n",
    "data_runtime = np.log(data[\"runtime\"] + 1 - min_runtime)\n",
    "data_resultsize = np.log(data[\"resultsize\"] + 1 - min_resultsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2015520f-280b-4973-98de-2c1cbc3e048e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32aba890-b2ef-439b-998c-ddd386e30c36",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## CNN1 Full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6dff28-bc56-48cb-9f1c-de6c48334cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"runs/CNN1Full/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embedding_dim = 32\n",
    "num_filters = 32\n",
    "filter_size = 9\n",
    "hidden_units = 128\n",
    "EPOCHS=200\n",
    "BATCH_SIZE = 1024\n",
    "OPTIMIZER = \"adamax\"\n",
    "\n",
    "il = Input(shape=(max_len,))\n",
    "el = Embedding(vocab_size, embedding_dim, input_length=max_len)(il)\n",
    "cl = Conv1D(num_filters, filter_size, activation='relu')(el)\n",
    "pl = GlobalMaxPooling1D()(cl)\n",
    "hl = Dense(hidden_units, activation='relu')(pl)\n",
    "outTime = Dense(1, name=\"outTime\")(hl)\n",
    "outSize = Dense(1, name=\"outSize\")(hl)\n",
    "model = Model(inputs=il, outputs=[outTime, outSize])\n",
    "print(model.summary())\n",
    "model.compile(optimizer=OPTIMIZER, loss={\"outTime\": 'huber', \"outSize\": \"huber\"}, metrics={'outTime': ['mae','mse'], 'outSize':['mae','mse']})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e9a1a8-0ca8-494a-bc4d-47fce1bea704",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## CNN1 Single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3651f5e4-b3bd-4315-a907-9d39a906b347",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"runs/CNN1Single/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embedding_dim = 32\n",
    "num_filters = 32\n",
    "filter_size = 9\n",
    "hidden_units = 128\n",
    "EPOCHS=200\n",
    "BATCH_SIZE = 1024\n",
    "OPTIMIZER = \"adamax\"\n",
    "\n",
    "il = Input(shape=(max_len,))\n",
    "el = Embedding(vocab_size, embedding_dim, input_length=max_len)(il)\n",
    "cl = Conv1D(num_filters, filter_size, activation='relu')(el)\n",
    "pl = GlobalMaxPooling1D()(cl)\n",
    "hl = Dense(hidden_units, activation='relu')(pl)\n",
    "out = Dense(1, name=\"out\")(hl)\n",
    "model = Model(inputs=il, outputs=out)\n",
    "print(model.summary())\n",
    "model.compile(optimizer=OPTIMIZER, loss='huber', metrics=['mae','mse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38ffdbc-6f46-4036-921f-9e7edeab823d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## CCN3 Full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f881953-9ec3-4877-8ea0-5c40d51f34bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"runs/ccnn3/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "EMBEDDING_DIM = 100\n",
    "filter_sizes = [3,4,5]\n",
    "num_filters = 512\n",
    "embedding_dim = 100\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "dropout_rate = 0.5\n",
    "EPOCHS=25\n",
    "BATCH_SIZE = 30\n",
    "OPTIMIZER = \"adamax\"\n",
    "\n",
    "inputs = Input(shape=(max_len,))\n",
    "embedding_layer = Embedding(vocab_size,\n",
    "                            EMBEDDING_DIM,\n",
    "                            # weights=[embedding_matrix],\n",
    "                            # trainable=False,\n",
    "                            input_length=max_len)(inputs)\n",
    "reshape = Reshape((max_len,EMBEDDING_DIM,1))(embedding_layer)\n",
    "\n",
    "conv_0 = Conv2D(num_filters, kernel_size=(filter_sizes[0], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "conv_1 = Conv2D(num_filters, kernel_size=(filter_sizes[1], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "conv_2 = Conv2D(num_filters, kernel_size=(filter_sizes[2], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "\n",
    "maxpool_0 = MaxPool2D(pool_size=(max_len - filter_sizes[0] + 1, 1), strides=(1,1), padding='valid')(conv_0)\n",
    "maxpool_1 = MaxPool2D(pool_size=(max_len - filter_sizes[1] + 1, 1), strides=(1,1), padding='valid')(conv_1)\n",
    "maxpool_2 = MaxPool2D(pool_size=(max_len - filter_sizes[2] + 1, 1), strides=(1,1), padding='valid')(conv_2)\n",
    "\n",
    "concatenated_tensor = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2])\n",
    "flatten = Flatten()(concatenated_tensor)\n",
    "dropout = Dropout(dropout_rate)(flatten)\n",
    "outTime = Dense(1, name=\"outTime\")(dropout)\n",
    "outSize = Dense(1, name=\"outSize\")(dropout)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=[outTime, outSize])\n",
    "\n",
    "model.compile(optimizer=OPTIMIZER, loss={\"outTime\": 'huber', \"outSize\": \"huber\"}, metrics={'outTime': ['mae','mse'], 'outSize':['mae','mse']})\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b134b788-8d9b-4768-955f-e98242d84046",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## CCN3 Single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cab9b4-a2a0-4a5b-8b66-a13036b95ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"runs/ccnn3Single/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "EMBEDDING_DIM = 100\n",
    "filter_sizes = [3,4,5]\n",
    "num_filters = 512\n",
    "embedding_dim = 100\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "dropout_rate = 0.5\n",
    "EPOCHS=250\n",
    "BATCH_SIZE = 512\n",
    "OPTIMIZER = \"adamax\"\n",
    "\n",
    "inputs = Input(shape=(max_len,))\n",
    "embedding_layer = Embedding(vocab_size,\n",
    "                            EMBEDDING_DIM,\n",
    "                            # weights=[embedding_matrix],\n",
    "                            # trainable=False,\n",
    "                            input_length=max_len)(inputs)\n",
    "reshape = Reshape((max_len,EMBEDDING_DIM,1))(embedding_layer)\n",
    "\n",
    "conv_0 = Conv2D(num_filters, kernel_size=(filter_sizes[0], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "conv_1 = Conv2D(num_filters, kernel_size=(filter_sizes[1], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "conv_2 = Conv2D(num_filters, kernel_size=(filter_sizes[2], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "\n",
    "maxpool_0 = MaxPool2D(pool_size=(max_len - filter_sizes[0] + 1, 1), strides=(1,1), padding='valid')(conv_0)\n",
    "maxpool_1 = MaxPool2D(pool_size=(max_len - filter_sizes[1] + 1, 1), strides=(1,1), padding='valid')(conv_1)\n",
    "maxpool_2 = MaxPool2D(pool_size=(max_len - filter_sizes[2] + 1, 1), strides=(1,1), padding='valid')(conv_2)\n",
    "\n",
    "concatenated_tensor = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2])\n",
    "flatten = Flatten()(concatenated_tensor)\n",
    "dropout = Dropout(dropout_rate)(flatten)\n",
    "out = Dense(1, name=\"out\")(dropout)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=out)\n",
    "\n",
    "model.compile(optimizer=OPTIMIZER, loss='huber', metrics=['mae','mse'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca813008-9d40-4daf-9d76-a30df86d400e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## VDCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43195c4-a502-42be-a4fc-2a34a4da217b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82657fde-0532-4cf8-bb53-95460b9ee5c0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49afeb06-c804-46a9-9c91-6f41d2299e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_BLOCKS = {9: (1, 1, 1, 1),\n",
    "            17: (2, 2, 2, 2),\n",
    "            29: (5, 5, 2, 2),\n",
    "            49: (8, 8, 5, 3)}\n",
    "\n",
    "\n",
    "class KMaxPooling(layers.Layer):\n",
    "    \"\"\"\n",
    "    K-max pooling layer that extracts the k-highest activations from a sequence (2nd dimension).\n",
    "    TensorFlow backend.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 k=None,\n",
    "                 sorted=False):\n",
    "        super(KMaxPooling, self).__init__()\n",
    "        self.k = k\n",
    "        self.sorted = sorted\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.k, input_shape[2])\n",
    "\n",
    "    def call(self,\n",
    "             inputs):\n",
    "        if self.k is None:\n",
    "            k = int(tf.round(inputs.shape[1] / 2))\n",
    "        else:\n",
    "            k = self.k\n",
    "\n",
    "        # Swap last two dimensions since top_k will be applied along the last dimension\n",
    "        shifted_inputs = tf.transpose(inputs, [0, 2, 1])\n",
    "\n",
    "        # Extract top_k, returns two tensors [values, indices]\n",
    "        top_k = tf.nn.top_k(shifted_inputs, k=k, sorted=self.sorted)[0]\n",
    "\n",
    "        # return flattened output\n",
    "        return tf.transpose(top_k, [0, 2, 1])\n",
    "\n",
    "\n",
    "class Pooling(layers.Layer):\n",
    "    \"\"\"Wrapper for different pooling operations.\n",
    "    Including maxpooling and k-maxpooling.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 pool_type='max',\n",
    "                 name=None):\n",
    "        super(Pooling, self).__init__(name=name)\n",
    "        assert pool_type in ['max', 'k_max']\n",
    "        self.pool_type = pool_type\n",
    "\n",
    "        if pool_type == 'max':\n",
    "            self.pool = layers.MaxPooling1D(pool_size=3, strides=2, padding='same')\n",
    "        elif pool_type == 'k_max':\n",
    "            self.pool = KMaxPooling()\n",
    "\n",
    "    def call(self,\n",
    "             x):\n",
    "        return self.pool(x)\n",
    "\n",
    "\n",
    "class ZeroPadding(layers.Layer):\n",
    "    def __init__(self,\n",
    "                 values,\n",
    "                 name=None):\n",
    "        super(ZeroPadding, self).__init__(name=name)\n",
    "        self.values = values\n",
    "\n",
    "    def call(self,\n",
    "             x):\n",
    "        x = tf.pad(x, [[0, 0], [0, 0], [self.values[0], self.values[1]]],\n",
    "                   mode='CONSTANT', constant_values=0)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Conv1D_BN(layers.Layer):\n",
    "    \"\"\"A stack of conv 1x1 and BatchNorm.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 filters,\n",
    "                 kernel_size=3,\n",
    "                 strides=2,\n",
    "                 padding='same',\n",
    "                 use_bias=True,\n",
    "                 name=None):\n",
    "        super(Conv1D_BN, self).__init__(name=name)\n",
    "        self.filters = filters\n",
    "        self.use_bias = use_bias\n",
    "        self.conv = layers.Conv1D(filters, kernel_size, strides=strides, padding=padding, use_bias=use_bias,\n",
    "                                  kernel_initializer='he_normal')\n",
    "        self.bn = layers.BatchNormalization()\n",
    "\n",
    "    def call(self,\n",
    "             x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ConvBlock(layers.Layer):\n",
    "    \"\"\"Conv block with downsampling.\n",
    "    1x1 conv to increase dimensions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 filters,\n",
    "                 kernel_size=3,\n",
    "                 use_bias=True,\n",
    "                 shortcut=True,\n",
    "                 pool_type=None,\n",
    "                 proj_type=None,\n",
    "                 name=None,\n",
    "                 ):\n",
    "        super(ConvBlock, self).__init__(name=name)\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.use_bias = use_bias\n",
    "        self.shortcut = shortcut\n",
    "        self.pool_type = pool_type\n",
    "        self.proj_type = proj_type\n",
    "\n",
    "        # Deal with downsample and pooling\n",
    "        assert pool_type in ['max', 'k_max', 'conv', None]\n",
    "        if pool_type is None:\n",
    "            strides = 1\n",
    "            self.pool = None\n",
    "            self.downsample = None\n",
    "\n",
    "        elif pool_type == 'conv':\n",
    "            strides = 2  # Convolutional pooling with stride 2\n",
    "            self.pool = None\n",
    "            if shortcut:\n",
    "                self.downsample = Conv1D_BN(filters, 3, strides=2, padding='same', use_bias=use_bias)\n",
    "\n",
    "        else:\n",
    "            strides = 1\n",
    "            self.pool = Pooling(pool_type)\n",
    "            if shortcut:\n",
    "                self.downsample = Conv1D_BN(filters, 3, strides=2, padding='same', use_bias=use_bias)\n",
    "\n",
    "        self.conv1 = layers.Conv1D(filters, kernel_size, strides=strides, padding='same', use_bias=use_bias,\n",
    "                                   kernel_initializer='he_normal')\n",
    "        self.bn1 = layers.BatchNormalization()\n",
    "\n",
    "        self.conv2 = layers.Conv1D(filters, kernel_size, strides=1, padding='same', use_bias=use_bias,\n",
    "                                   kernel_initializer='he_normal')\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "\n",
    "        assert proj_type in ['identity', 'conv', None]\n",
    "        if shortcut:\n",
    "            if proj_type == 'conv':\n",
    "                # 1x1 conv for projection\n",
    "                self.proj = Conv1D_BN(filters * 2, 1, strides=1, padding='same', use_bias=use_bias)\n",
    "\n",
    "            elif proj_type == 'identity':\n",
    "                # Identity using zero padding\n",
    "                self.proj = ZeroPadding([int(filters // 2), filters - int(filters // 2)])\n",
    "\n",
    "    def call(self,\n",
    "             x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = tf.nn.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.pool is not None:\n",
    "            out = self.pool(out)\n",
    "\n",
    "        if self.shortcut:\n",
    "            if self.downsample is not None:\n",
    "                residual = self.downsample(residual)\n",
    "            out += residual\n",
    "\n",
    "        out = tf.nn.relu(out)\n",
    "\n",
    "        if self.proj_type is not None and self.shortcut:\n",
    "            out = self.proj(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class VDCNN(Model):\n",
    "    \"\"\"Model codebase for VDCNN.\n",
    "    Args:\n",
    "        depth: depth of VDCNN, one of [9, 17, 29, 49].\n",
    "        seqlen: Sequence length.\n",
    "        embed_dim: dim for character embeddings.\n",
    "        shortcut: Use skip connections.\n",
    "        pool_type: Pooling operations to be used, one of ['max', 'k_max', 'conv'].\n",
    "        proj_type: Operation to increase dim for dotted skip connection, one of ['identity', 'conv'].\n",
    "        use_bias: Use bias for all layers or not.\n",
    "        logits: If False, return softmax probs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 depth=9,\n",
    "                 vocab_size=69,\n",
    "                 seqlen=None,\n",
    "                 embed_dim=16,\n",
    "                 shortcut=True,\n",
    "                 pool_type='max',\n",
    "                 proj_type='conv',\n",
    "                 use_bias=True,\n",
    "                 logits=True):\n",
    "        super(VDCNN, self).__init__()\n",
    "        self.depth = depth\n",
    "        self.vocab_size = vocab_size\n",
    "        self.seqlen = seqlen\n",
    "        self.embed_dim = embed_dim\n",
    "        self.shortcut = shortcut\n",
    "        self.pool_type = pool_type\n",
    "        self.proj_type = proj_type\n",
    "        self.use_bias = use_bias\n",
    "        self.logits = True\n",
    "\n",
    "        assert pool_type in ['max', 'k_max', 'conv']\n",
    "        assert proj_type in ['conv', 'identity']\n",
    "        self.n_blocks = N_BLOCKS[depth]\n",
    "\n",
    "        self.embed_char = layers.Embedding(vocab_size, embed_dim, input_length=seqlen)\n",
    "        self.conv = layers.Conv1D(64, 3, strides=1, padding='same', use_bias=use_bias,\n",
    "                                  kernel_initializer='he_normal')\n",
    "\n",
    "        # Convolutional Block 64\n",
    "        self.conv_block_64 = []\n",
    "        for _ in range(self.n_blocks[0] - 1):\n",
    "            self.conv_block_64.append(ConvBlock(64, 3, use_bias, shortcut))\n",
    "        self.conv_block_64.append(ConvBlock(64, 3, use_bias, shortcut, pool_type=pool_type, proj_type=proj_type))\n",
    "\n",
    "        # Convolutional Block 128\n",
    "        self.conv_block_128 = []\n",
    "        for _ in range(self.n_blocks[1] - 1):\n",
    "            self.conv_block_128.append(ConvBlock(128, 3, use_bias, shortcut))\n",
    "        self.conv_block_128.append(ConvBlock(128, 3, use_bias, shortcut, pool_type=pool_type, proj_type=proj_type))\n",
    "\n",
    "        # Convolutional Block 256\n",
    "        self.conv_block_256 = []\n",
    "        for _ in range(self.n_blocks[2] - 1):\n",
    "            self.conv_block_256.append(ConvBlock(256, 3, use_bias, shortcut))\n",
    "        self.conv_block_256.append(ConvBlock(256, 3, use_bias, shortcut, pool_type=pool_type, proj_type=proj_type))\n",
    "\n",
    "        # Convolutional Block 512\n",
    "        self.conv_block_512 = []\n",
    "        for _ in range(self.n_blocks[3] - 1):\n",
    "            self.conv_block_512.append(ConvBlock(512, 3, use_bias, shortcut))\n",
    "        self.conv_block_512.append(ConvBlock(512, 3, use_bias, shortcut, pool_type=None, proj_type=None))\n",
    "\n",
    "        self.k_maxpool = KMaxPooling(k=8)\n",
    "        self.flatten = layers.Flatten()\n",
    "\n",
    "        # Dense layers\n",
    "        self.fc1 = layers.Dense(2048, activation='relu')\n",
    "        self.fc2 = layers.Dense(2048, activation='relu')\n",
    "        self.outTime = layers.Dense(1, name=\"outTime\")\n",
    "        self.outSize = layers.Dense(1, name=\"outSize\")\n",
    "        # self.out = layers.Dense(2)\n",
    "\n",
    "    def call(self,\n",
    "             x):\n",
    "        x = self.embed_char(x)\n",
    "        # print('embed:', x.shape)\n",
    "        x = self.conv(x)\n",
    "        # print('conv:', x.shape)\n",
    "\n",
    "        for l in self.conv_block_64:\n",
    "            x = l(x)\n",
    "        # print('conv_block_64:', x.shape)\n",
    "\n",
    "        for l in self.conv_block_128:\n",
    "            x = l(x)\n",
    "        # print('conv_block_128:', x.shape)\n",
    "\n",
    "        for l in self.conv_block_256:\n",
    "            x = l(x)\n",
    "        # print('conv_block_256:', x.shape)\n",
    "\n",
    "        for l in self.conv_block_512:\n",
    "            x = l(x)\n",
    "        # print('conv_block_512:', x.shape)\n",
    "\n",
    "        x = self.k_maxpool(x)\n",
    "        # print('k_maxpool_8:', x.shape)\n",
    "        x = self.flatten(x)\n",
    "        # print('flatten:', x.shape)\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        outTime = self.outTime(x)\n",
    "        outSize = self.outSize(x)\n",
    "        # out = self.out(x)\n",
    "        # print('out:', out.shape)\n",
    "\n",
    "        return outTime, outSize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1031741a-542f-4c87-83a6-019713a53188",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d095ac78-d85f-45a8-b144-c153238d7bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_BLOCKS = {9: (1, 1, 1, 1),\n",
    "            17: (2, 2, 2, 2),\n",
    "            29: (5, 5, 2, 2),\n",
    "            49: (8, 8, 5, 3)}\n",
    "\n",
    "\n",
    "class KMaxPooling(layers.Layer):\n",
    "    \"\"\"\n",
    "    K-max pooling layer that extracts the k-highest activations from a sequence (2nd dimension).\n",
    "    TensorFlow backend.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 k=None,\n",
    "                 sorted=False):\n",
    "        super(KMaxPooling, self).__init__()\n",
    "        self.k = k\n",
    "        self.sorted = sorted\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.k, input_shape[2])\n",
    "\n",
    "    def call(self,\n",
    "             inputs):\n",
    "        if self.k is None:\n",
    "            k = int(tf.round(inputs.shape[1] / 2))\n",
    "        else:\n",
    "            k = self.k\n",
    "\n",
    "        # Swap last two dimensions since top_k will be applied along the last dimension\n",
    "        shifted_inputs = tf.transpose(inputs, [0, 2, 1])\n",
    "\n",
    "        # Extract top_k, returns two tensors [values, indices]\n",
    "        top_k = tf.nn.top_k(shifted_inputs, k=k, sorted=self.sorted)[0]\n",
    "\n",
    "        # return flattened output\n",
    "        return tf.transpose(top_k, [0, 2, 1])\n",
    "\n",
    "\n",
    "class Pooling(layers.Layer):\n",
    "    \"\"\"Wrapper for different pooling operations.\n",
    "    Including maxpooling and k-maxpooling.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 pool_type='max',\n",
    "                 name=None):\n",
    "        super(Pooling, self).__init__(name=name)\n",
    "        assert pool_type in ['max', 'k_max']\n",
    "        self.pool_type = pool_type\n",
    "\n",
    "        if pool_type == 'max':\n",
    "            self.pool = layers.MaxPooling1D(pool_size=3, strides=2, padding='same')\n",
    "        elif pool_type == 'k_max':\n",
    "            self.pool = KMaxPooling()\n",
    "\n",
    "    def call(self,\n",
    "             x):\n",
    "        return self.pool(x)\n",
    "\n",
    "\n",
    "class ZeroPadding(layers.Layer):\n",
    "    def __init__(self,\n",
    "                 values,\n",
    "                 name=None):\n",
    "        super(ZeroPadding, self).__init__(name=name)\n",
    "        self.values = values\n",
    "\n",
    "    def call(self,\n",
    "             x):\n",
    "        x = tf.pad(x, [[0, 0], [0, 0], [self.values[0], self.values[1]]],\n",
    "                   mode='CONSTANT', constant_values=0)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Conv1D_BN(layers.Layer):\n",
    "    \"\"\"A stack of conv 1x1 and BatchNorm.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 filters,\n",
    "                 kernel_size=3,\n",
    "                 strides=2,\n",
    "                 padding='same',\n",
    "                 use_bias=True,\n",
    "                 name=None):\n",
    "        super(Conv1D_BN, self).__init__(name=name)\n",
    "        self.filters = filters\n",
    "        self.use_bias = use_bias\n",
    "        self.conv = layers.Conv1D(filters, kernel_size, strides=strides, padding=padding, use_bias=use_bias,\n",
    "                                  kernel_initializer='he_normal')\n",
    "        self.bn = layers.BatchNormalization()\n",
    "\n",
    "    def call(self,\n",
    "             x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ConvBlock(layers.Layer):\n",
    "    \"\"\"Conv block with downsampling.\n",
    "    1x1 conv to increase dimensions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 filters,\n",
    "                 kernel_size=3,\n",
    "                 use_bias=True,\n",
    "                 shortcut=True,\n",
    "                 pool_type=None,\n",
    "                 proj_type=None,\n",
    "                 name=None,\n",
    "                 ):\n",
    "        super(ConvBlock, self).__init__(name=name)\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.use_bias = use_bias\n",
    "        self.shortcut = shortcut\n",
    "        self.pool_type = pool_type\n",
    "        self.proj_type = proj_type\n",
    "\n",
    "        # Deal with downsample and pooling\n",
    "        assert pool_type in ['max', 'k_max', 'conv', None]\n",
    "        if pool_type is None:\n",
    "            strides = 1\n",
    "            self.pool = None\n",
    "            self.downsample = None\n",
    "\n",
    "        elif pool_type == 'conv':\n",
    "            strides = 2  # Convolutional pooling with stride 2\n",
    "            self.pool = None\n",
    "            if shortcut:\n",
    "                self.downsample = Conv1D_BN(filters, 3, strides=2, padding='same', use_bias=use_bias)\n",
    "\n",
    "        else:\n",
    "            strides = 1\n",
    "            self.pool = Pooling(pool_type)\n",
    "            if shortcut:\n",
    "                self.downsample = Conv1D_BN(filters, 3, strides=2, padding='same', use_bias=use_bias)\n",
    "\n",
    "        self.conv1 = layers.Conv1D(filters, kernel_size, strides=strides, padding='same', use_bias=use_bias,\n",
    "                                   kernel_initializer='he_normal')\n",
    "        self.bn1 = layers.BatchNormalization()\n",
    "\n",
    "        self.conv2 = layers.Conv1D(filters, kernel_size, strides=1, padding='same', use_bias=use_bias,\n",
    "                                   kernel_initializer='he_normal')\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "\n",
    "        assert proj_type in ['identity', 'conv', None]\n",
    "        if shortcut:\n",
    "            if proj_type == 'conv':\n",
    "                # 1x1 conv for projection\n",
    "                self.proj = Conv1D_BN(filters * 2, 1, strides=1, padding='same', use_bias=use_bias)\n",
    "\n",
    "            elif proj_type == 'identity':\n",
    "                # Identity using zero padding\n",
    "                self.proj = ZeroPadding([int(filters // 2), filters - int(filters // 2)])\n",
    "\n",
    "    def call(self,\n",
    "             x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = tf.nn.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.pool is not None:\n",
    "            out = self.pool(out)\n",
    "\n",
    "        if self.shortcut:\n",
    "            if self.downsample is not None:\n",
    "                residual = self.downsample(residual)\n",
    "            out += residual\n",
    "\n",
    "        out = tf.nn.relu(out)\n",
    "\n",
    "        if self.proj_type is not None and self.shortcut:\n",
    "            out = self.proj(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class VDCNN(Model):\n",
    "    \"\"\"Model codebase for VDCNN.\n",
    "    Args:\n",
    "        depth: depth of VDCNN, one of [9, 17, 29, 49].\n",
    "        seqlen: Sequence length.\n",
    "        embed_dim: dim for character embeddings.\n",
    "        shortcut: Use skip connections.\n",
    "        pool_type: Pooling operations to be used, one of ['max', 'k_max', 'conv'].\n",
    "        proj_type: Operation to increase dim for dotted skip connection, one of ['identity', 'conv'].\n",
    "        use_bias: Use bias for all layers or not.\n",
    "        logits: If False, return softmax probs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 depth=9,\n",
    "                 vocab_size=69,\n",
    "                 seqlen=None,\n",
    "                 embed_dim=16,\n",
    "                 shortcut=True,\n",
    "                 pool_type='max',\n",
    "                 proj_type='conv',\n",
    "                 use_bias=True,\n",
    "                 logits=True):\n",
    "        super(VDCNN, self).__init__()\n",
    "        self.depth = depth\n",
    "        self.vocab_size = vocab_size\n",
    "        self.seqlen = seqlen\n",
    "        self.embed_dim = embed_dim\n",
    "        self.shortcut = shortcut\n",
    "        self.pool_type = pool_type\n",
    "        self.proj_type = proj_type\n",
    "        self.use_bias = use_bias\n",
    "        self.logits = True\n",
    "\n",
    "        assert pool_type in ['max', 'k_max', 'conv']\n",
    "        assert proj_type in ['conv', 'identity']\n",
    "        self.n_blocks = N_BLOCKS[depth]\n",
    "\n",
    "        self.embed_char = layers.Embedding(vocab_size, embed_dim, input_length=seqlen)\n",
    "        self.conv = layers.Conv1D(64, 3, strides=1, padding='same', use_bias=use_bias,\n",
    "                                  kernel_initializer='he_normal')\n",
    "\n",
    "        # Convolutional Block 64\n",
    "        self.conv_block_64 = []\n",
    "        for _ in range(self.n_blocks[0] - 1):\n",
    "            self.conv_block_64.append(ConvBlock(64, 3, use_bias, shortcut))\n",
    "        self.conv_block_64.append(ConvBlock(64, 3, use_bias, shortcut, pool_type=pool_type, proj_type=proj_type))\n",
    "\n",
    "        # Convolutional Block 128\n",
    "        self.conv_block_128 = []\n",
    "        for _ in range(self.n_blocks[1] - 1):\n",
    "            self.conv_block_128.append(ConvBlock(128, 3, use_bias, shortcut))\n",
    "        self.conv_block_128.append(ConvBlock(128, 3, use_bias, shortcut, pool_type=pool_type, proj_type=proj_type))\n",
    "\n",
    "        # Convolutional Block 256\n",
    "        self.conv_block_256 = []\n",
    "        for _ in range(self.n_blocks[2] - 1):\n",
    "            self.conv_block_256.append(ConvBlock(256, 3, use_bias, shortcut))\n",
    "        self.conv_block_256.append(ConvBlock(256, 3, use_bias, shortcut, pool_type=pool_type, proj_type=proj_type))\n",
    "\n",
    "        # Convolutional Block 512\n",
    "        self.conv_block_512 = []\n",
    "        for _ in range(self.n_blocks[3] - 1):\n",
    "            self.conv_block_512.append(ConvBlock(512, 3, use_bias, shortcut))\n",
    "        self.conv_block_512.append(ConvBlock(512, 3, use_bias, shortcut, pool_type=None, proj_type=None))\n",
    "\n",
    "        self.k_maxpool = KMaxPooling(k=8)\n",
    "        self.flatten = layers.Flatten()\n",
    "\n",
    "        # Dense layers\n",
    "        self.fc1 = layers.Dense(2048, activation='relu')\n",
    "        self.fc2 = layers.Dense(2048, activation='relu')\n",
    "        self.out = layers.Dense(1, name=\"out\")\n",
    "        # self.out = layers.Dense(2)\n",
    "\n",
    "    def call(self,\n",
    "             x):\n",
    "        x = self.embed_char(x)\n",
    "        # print('embed:', x.shape)\n",
    "        x = self.conv(x)\n",
    "        # print('conv:', x.shape)\n",
    "\n",
    "        for l in self.conv_block_64:\n",
    "            x = l(x)\n",
    "        # print('conv_block_64:', x.shape)\n",
    "\n",
    "        for l in self.conv_block_128:\n",
    "            x = l(x)\n",
    "        # print('conv_block_128:', x.shape)\n",
    "\n",
    "        for l in self.conv_block_256:\n",
    "            x = l(x)\n",
    "        # print('conv_block_256:', x.shape)\n",
    "\n",
    "        for l in self.conv_block_512:\n",
    "            x = l(x)\n",
    "        # print('conv_block_512:', x.shape)\n",
    "\n",
    "        x = self.k_maxpool(x)\n",
    "        # print('k_maxpool_8:', x.shape)\n",
    "        x = self.flatten(x)\n",
    "        # print('flatten:', x.shape)\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        out = self.out(x)\n",
    "        # outSize = self.outSize(x)\n",
    "        # out = self.out(x)\n",
    "        # print('out:', out.shape)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2157490c-e9b9-4520-b103-9d34a7aa19fb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d35b18f-7e83-4e62-92fa-a81d07d31126",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"runs/vdcnn/depth9/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# Hyperparameters\n",
    "DEPTH = 9\n",
    "EMBED_DIM = 256\n",
    "SHORTCUT = True\n",
    "POOL_TYPE = 'k_max'\n",
    "PROJ_TYPE = 'conv'\n",
    "USE_BIAS = False\n",
    "OPTIMIZER = \"adam\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecc4c65-c1e9-436b-9303-a1c737cecaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"runs/vdcnn/depth17/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# Hyperparameters\n",
    "DEPTH = 17\n",
    "EMBED_DIM = 256\n",
    "SHORTCUT = 'True'\n",
    "POOL_TYPE = 'k_max'\n",
    "PROJ_TYPE = 'identity'\n",
    "USE_BIAS = True\n",
    "OPTIMIZER = 'adam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86643304-6630-4b75-bdf1-711d51ba148a",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"runs/vdcnn/depth29/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# Hyperparameters\n",
    "DEPTH = 29\n",
    "EMBED_DIM = 256\n",
    "SHORTCUT = 'True'\n",
    "POOL_TYPE = 'k_max'\n",
    "PROJ_TYPE = 'identity'\n",
    "USE_BIAS = True\n",
    "OPTIMIZER = 'adam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80924745-47e8-43bf-97c7-c1b091b3e371",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"runs/vdcnn/depth49/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# Hyperparameters\n",
    "DEPTH = 49\n",
    "EMBED_DIM = 256\n",
    "SHORTCUT = 'True'\n",
    "POOL_TYPE = 'conv'\n",
    "PROJ_TYPE = 'conv'\n",
    "USE_BIAS = False\n",
    "OPTIMIZER = 'adam'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8551e7dd-2b25-4fa9-864f-a8d917b88eb9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Compiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92604ad7-b538-4dc6-b2b1-212e06bd9661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 50\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Model\n",
    "model = VDCNN(depth=DEPTH,\n",
    "              vocab_size=vocab_size,\n",
    "              seqlen=max_len,\n",
    "              embed_dim=EMBED_DIM,\n",
    "              shortcut=SHORTCUT,\n",
    "              pool_type=POOL_TYPE,\n",
    "              proj_type=PROJ_TYPE,\n",
    "              use_bias=USE_BIAS)\n",
    "model.compile(optimizer=OPTIMIZER, loss={\"output_1\": 'huber', \"output_2\": \"huber\"}, metrics={'output_1': ['mae','mse'], 'output_2':['mae','mse']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b3c541-d177-4438-b0ac-154076b3914f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 200\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Model\n",
    "model = VDCNN(depth=DEPTH,\n",
    "              vocab_size=vocab_size,\n",
    "              seqlen=max_len,\n",
    "              embed_dim=EMBED_DIM,\n",
    "              shortcut=SHORTCUT,\n",
    "              pool_type=POOL_TYPE,\n",
    "              proj_type=PROJ_TYPE,\n",
    "              use_bias=USE_BIAS)\n",
    "model.compile(optimizer=OPTIMIZER, loss='huber', metrics=['mae','mse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4e25d4-57dc-45c5-ac50-76a0c751e5e7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ccb375-d469-43ff-8e53-617f0d99dfc2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755a84e3-7b62-4989-baf8-3d0f9d087926",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(padded, [data_runtime, data_resultsize],\n",
    "          epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=2,\n",
    "      callbacks=[tensorboard_callback],\n",
    "      shuffle=False,\n",
    "      validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4f90c2-0466-4ef4-a8e1-01bc59c08d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"models/CNN1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602bdb3e-1df1-4e1c-951e-d7c25decab0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"models/vdcnn49\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c93e76-e680-4b95-b366-1e7f9cb5550d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7701a02a-ecbf-415d-9cb7-ea744daf2738",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(padded, data_runtime, \n",
    "          epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=2,\n",
    "      callbacks=[tensorboard_callback],\n",
    "      shuffle=False,\n",
    "      validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55ae405-0e61-44a4-a3ec-3417b012ecd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"models/vdcnn49Rt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce1553e-81ad-44b8-b047-f8f0b433b291",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"models/ccnn3Rt.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d56205-44b9-49e8-ad7d-489cc4db214f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Resultsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1489369e-56ea-47b8-bff3-fcdb1a95f55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(padded, data_resultsize,\n",
    "          epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=2,\n",
    "      callbacks=[tensorboard_callback],\n",
    "      shuffle=False,\n",
    "      validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8b3e6f-0164-446b-92a3-8163a9b18394",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"models/vdcnn17Card\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1af66a-fd27-42d0-852a-f4aed279e102",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"models/ccnn3Card.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7cff92-719e-4769-b791-201f89e2ce42",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b3f94c-79f9-476d-84a7-cc66790039bd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb52d3f-6050-4cb2-b111-4dbfe2c8c596",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"models/CNN1Card.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d186bfb6-d423-4434-98fb-91b361081db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"models/vdcnn49Rt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba782b84-246b-4029-8538-4f75fd21380e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Load Evaluation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e75e1d-7816-4762-9723-5daea77c4ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(logging)\n",
    "logging.basicConfig(filename='logs/ccnn.log', level=logging.DEBUG, format=\"%(asctime)s    %(message)s\",\n",
    "                              datefmt=\"%H:%M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa0e504-6691-4cd9-ac99-03c878211f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(logging)\n",
    "logging.basicConfig(filename='logs/vdcnn49.log', level=logging.DEBUG, format=\"%(asctime)s    %(message)s\",\n",
    "                              datefmt=\"%H:%M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "16cb4bd7-ef82-43bc-b710-db316af27f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3315\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model on test data\n",
    "data = pd.read_csv(f\"{path}test.csv\", on_bad_lines=\"skip\", encoding=\"latin-1\", lineterminator=\"\\n\",\n",
    "                                dtype={\"statement\": str, \"runtime\": float, \"resultsize\": int, \"yy\": int,\n",
    "                                       \"mm:\": int, \"dd\": int}, memory_map=True)\n",
    "print(len(data))\n",
    "# Tokenize input SQL statements\n",
    "tokenizer = Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(data[\"statement\"])\n",
    "\n",
    "# Pad input sequences\n",
    "max_len = 512\n",
    "sequences = tokenizer.texts_to_sequences(data[\"statement\"])\n",
    "padded = pad_sequences(sequences, maxlen=max_len, padding='post', truncating='post')\n",
    "\n",
    "min_runtime = min(data[\"runtime\"])\n",
    "min_resultsize = min(data[\"resultsize\"])\n",
    "\n",
    "data_runtime = np.log(data[\"runtime\"] + 1 - min_runtime)\n",
    "data_resultsize = np.log(data[\"resultsize\"] + 1 - min_resultsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890809e0-7029-4fd4-bb16-f34ec5fe3600",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558527b3-be90-4ae2-bf59-2f6711bdc089",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.evaluate(padded, [data_runtime, data_resultsize], return_dict=True) # [data_runtime, data_resultsize]\n",
    "# logging.info(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9e6d0d-7b2f-4ac5-a828-356921835226",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(padded)\n",
    "\n",
    "# convert to actual predictions\n",
    "pred_time = [np.exp(x[0])-1+min_runtime for x in preds[0]]\n",
    "pred_size = [np.exp(x[0])-1+min_resultsize for x in preds[1]]\n",
    "\n",
    "qerror_time = [max(pred_time[i] / data[\"runtime\"][i], data[\"runtime\"][i] / pred_time[i]) for i in range(len(data[\"runtime\"]))]\n",
    "qerror_size = [max(max(pred_size[i],1) / max(1,data[\"resultsize\"][i]), max(1,data[\"resultsize\"][i]) / max(1,pred_size[i])) for i in range(len(data[\"resultsize\"]))]\n",
    "\n",
    "for (qerror,name) in [(qerror_time,\"runtime\"), (qerror_size,\"resultsize\")]:\n",
    "    logging.info(\"\")\n",
    "    logging.info(f\"Qerror for {name}\")\n",
    "    logging.info(\"Median: {}\".format(np.median(qerror)))\n",
    "    logging.info(\"Mean: {}\".format(np.mean(qerror)))\n",
    "    logging.info(\"Max: {}\".format(np.max(qerror)))\n",
    "    logging.info(\"10th percentile: {}\".format(np.percentile(qerror, 10)))\n",
    "    logging.info(\"20th percentile: {}\".format(np.percentile(qerror, 20)))\n",
    "    logging.info(\"30th percentile: {}\".format(np.percentile(qerror, 30)))\n",
    "    logging.info(\"40th percentile: {}\".format(np.percentile(qerror, 40)))\n",
    "    logging.info(\"50th percentile: {}\".format(np.percentile(qerror, 50)))\n",
    "    logging.info(\"60th percentile: {}\".format(np.percentile(qerror, 60)))\n",
    "    logging.info(\"70th percentile: {}\".format(np.percentile(qerror, 70)))\n",
    "    logging.info(\"80th percentile: {}\".format(np.percentile(qerror, 80)))\n",
    "    logging.info(\"90th percentile: {}\".format(np.percentile(qerror, 90)))\n",
    "    logging.info(\"95th percentile: {}\".format(np.percentile(qerror, 95)))\n",
    "    logging.info(\"98th percentile: {}\".format(np.percentile(qerror, 98)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcc6ee8-dc9c-40e0-9b4e-a5a14f42f91f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Evaluation Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac71dd76-c0f4-41f9-8e06-b655e46375dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.evaluate(padded, data_runtime, return_dict=True)\n",
    "logging.info(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd747a31-2db2-49fd-8bed-d2a2fbb819d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(padded)\n",
    "\n",
    "# convert to actual predictions\n",
    "pred_time = [np.exp(x[0])-1+min_runtime for x in preds]\n",
    "qerror = [max(pred_time[i] / data[\"runtime\"][i], data[\"runtime\"][i] / pred_time[i]) for i in range(len(data[\"runtime\"]))]\n",
    "\n",
    "logging.info(\"\")\n",
    "logging.info(f\"Qerror for runtime\")\n",
    "logging.info(\"Median: {}\".format(np.median(qerror)))\n",
    "logging.info(\"Mean: {}\".format(np.mean(qerror)))\n",
    "logging.info(\"Max: {}\".format(np.max(qerror)))\n",
    "logging.info(\"10th percentile: {}\".format(np.percentile(qerror, 10)))\n",
    "logging.info(\"20th percentile: {}\".format(np.percentile(qerror, 20)))\n",
    "logging.info(\"30th percentile: {}\".format(np.percentile(qerror, 30)))\n",
    "logging.info(\"40th percentile: {}\".format(np.percentile(qerror, 40)))\n",
    "logging.info(\"50th percentile: {}\".format(np.percentile(qerror, 50)))\n",
    "logging.info(\"60th percentile: {}\".format(np.percentile(qerror, 60)))\n",
    "logging.info(\"70th percentile: {}\".format(np.percentile(qerror, 70)))\n",
    "logging.info(\"80th percentile: {}\".format(np.percentile(qerror, 80)))\n",
    "logging.info(\"90th percentile: {}\".format(np.percentile(qerror, 90)))\n",
    "logging.info(\"95th percentile: {}\".format(np.percentile(qerror, 95)))\n",
    "logging.info(\"98th percentile: {}\".format(np.percentile(qerror, 98)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185690c9-bc84-4a13-b93f-c1fca6feec7b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Evaluation Resultsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1505a6f-0373-479c-9bc7-2697052bc10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.evaluate(padded, data_resultsize, return_dict=True)\n",
    "logging.info(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc42c33-28e4-4e52-a93a-f7f1f0a42d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(padded)\n",
    "\n",
    "# convert to actual predictions\n",
    "pred_size = [np.exp(x[0])-1+min_resultsize for x in preds]\n",
    "qerror = [max(max(pred_size[i],1) / max(1,data[\"resultsize\"][i]), max(1,data[\"resultsize\"][i]) / max(1,pred_size[i])) for i in range(len(data[\"resultsize\"]))]\n",
    "\n",
    "logging.info(\"\")\n",
    "logging.info(f\"Qerror for resultsize\")\n",
    "logging.info(\"Median: {}\".format(np.median(qerror)))\n",
    "logging.info(\"Mean: {}\".format(np.mean(qerror)))\n",
    "logging.info(\"Max: {}\".format(np.max(qerror)))\n",
    "logging.info(\"10th percentile: {}\".format(np.percentile(qerror, 10)))\n",
    "logging.info(\"20th percentile: {}\".format(np.percentile(qerror, 20)))\n",
    "logging.info(\"30th percentile: {}\".format(np.percentile(qerror, 30)))\n",
    "logging.info(\"40th percentile: {}\".format(np.percentile(qerror, 40)))\n",
    "logging.info(\"50th percentile: {}\".format(np.percentile(qerror, 50)))\n",
    "logging.info(\"60th percentile: {}\".format(np.percentile(qerror, 60)))\n",
    "logging.info(\"70th percentile: {}\".format(np.percentile(qerror, 70)))\n",
    "logging.info(\"80th percentile: {}\".format(np.percentile(qerror, 80)))\n",
    "logging.info(\"90th percentile: {}\".format(np.percentile(qerror, 90)))\n",
    "logging.info(\"95th percentile: {}\".format(np.percentile(qerror, 95)))\n",
    "logging.info(\"98th percentile: {}\".format(np.percentile(qerror, 98)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1de5950-5d49-48e3-8ace-b28329776f99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
